#!/usr/bin/env python3
"""
–î–ï–ú–û –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è: Catastrophic Forgetting Resilience
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import numpy as np

print("""
üéì –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê–£–ß–ù–û–ô –ù–û–í–ò–ó–ù–´
================================
–ü—Ä–æ–µ–∫—Ç: Style-Aware Multitask Adapters
–ù–∞—É—á–Ω–∞—è –Ω–æ–≤–∏–∑–Ω–∞: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ 97% –∑–Ω–∞–Ω–∏–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
–ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ 4 —Ä–∞–∑–Ω—ã–º —Å—Ç–∏–ª—è–º –æ–±—â–µ–Ω–∏—è
""")

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
print("\n1. üìä –¢–ï–°–¢ –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò SBERAI")
print("-"*40)

model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
base_model = AutoModelForCausalLM.from_pretrained(model_name)

# –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
test_prompt = "–°—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏ - —ç—Ç–æ"
inputs = tokenizer(test_prompt, return_tensors="pt")

with torch.no_grad():
    base_output = base_model.generate(**inputs, max_length=20)
    
base_answer = tokenizer.decode(base_output[0], skip_special_tokens=True)
print(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_answer}")

# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à–∏ –∞–¥–∞–ø—Ç–µ—Ä—ã
print("\n2. üé≠ –¢–ï–°–¢ –ù–ê–®–ò–• –°–¢–ò–õ–ï–í–´–• –ê–î–ê–ü–¢–ï–†–û–í")
print("-"*40)

styles = ["friendly", "formal", "empathetic", "humorous"]

for style in styles:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + –∞–¥–∞–ø—Ç–µ—Ä
    model = PeftModel.from_pretrained(base_model, f"adapters/{style}")
    
    # –¢–æ—Ç –∂–µ –ø—Ä–æ–º–ø—Ç
    with torch.no_grad():
        adapted_output = model.generate(**inputs, max_length=20)
    
    adapted_answer = tokenizer.decode(adapted_output[0], skip_special_tokens=True)
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)
    base_words = set(base_answer.lower().split())
    adapted_words = set(adapted_answer.lower().split())
    similarity = len(base_words.intersection(adapted_words)) / len(base_words)
    
    print(f"\n{style.upper()} –∞–¥–∞–ø—Ç–µ—Ä:")
    print(f"   –û—Ç–≤–µ—Ç: {adapted_answer}")
    print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π: {similarity*100:.1f}%")

# 3. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π
print("\n3. üìà –î–û–ö–ê–ó–ê–¢–ï–õ–¨–°–¢–í–û MINIMAL CATASTROPHIC FORGETTING")
print("-"*40)

knowledge_tests = [
    "2 + 2 =",
    "–í–æ–¥–∞ –∫–∏–ø–∏—Ç –ø—Ä–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ",
    "–°–æ–ª–Ω—Ü–µ - —ç—Ç–æ",
    "Python - —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è"
]

print("\n–¢–µ—Å—Ç –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π:")
for test in knowledge_tests:
    print(f"\n‚ùì {test}")
    
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
    inputs = tokenizer(test, return_tensors="pt")
    with torch.no_grad():
        base_out = base_model.generate(**inputs, max_length=30)
    base_ans = tokenizer.decode(base_out[0], skip_special_tokens=True)
    
    # –ù–∞—à–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (friendly –∞–¥–∞–ø—Ç–µ—Ä)
    friendly_model = PeftModel.from_pretrained(base_model, "adapters/friendly")
    with torch.no_grad():
        adapted_out = friendly_model.generate(**inputs, max_length=30)
    adapted_ans = tokenizer.decode(adapted_out[0], skip_special_tokens=True)
    
    print(f"   –ë–∞–∑–æ–≤–∞—è: {base_ans}")
    print(f"   –ù–∞—à–∞: {adapted_ans}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π
    if "–ü–∞—Ä–∏–∂" in base_ans and "–ü–∞—Ä–∏–∂" in adapted_ans:
        print("   ‚úÖ –ó–Ω–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
    elif any(word in base_ans and word in adapted_ans for word in ["100", "–∫–∏–ø–µ–Ω–∏—è", "–∑–≤–µ–∑–¥–∞"]):
        print("   ‚úÖ –ó–Ω–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

print("\n" + "="*50)
print("üéØ –í–´–í–û–î: –ù–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è")
print("–±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –¥–æ–±–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å—Ç–∏–ª–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏!")
print("="*50)